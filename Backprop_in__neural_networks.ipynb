{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5BC9qT2RW1dd9+cMrnAsW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugam24/Coding-Neural-Networks-from-scratch/blob/main/Backprop_in__neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Back propagation in neural network"
      ],
      "metadata": {
        "id": "Vkv1bvlVfNvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back propagation in single neuron"
      ],
      "metadata": {
        "id": "GSH46VW4c9QO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIGP2Ox2fC9q",
        "outputId": "cae80662-b1d9-40bb-f581-09aa9b6740bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, Loss: 36.0\n",
            "Iteration 2, Loss: 33.872399999999985\n",
            "Iteration 3, Loss: 31.870541159999995\n",
            "Iteration 4, Loss: 29.98699217744401\n",
            "Iteration 5, Loss: 28.21476093975706\n",
            "Iteration 6, Loss: 26.54726856821742\n",
            "Iteration 7, Loss: 24.978324995835766\n",
            "Iteration 8, Loss: 23.502105988581878\n",
            "Iteration 9, Loss: 22.113131524656684\n",
            "Iteration 10, Loss: 20.80624545154949\n",
            "Iteration 11, Loss: 19.576596345362915\n",
            "Iteration 12, Loss: 18.419619501351963\n",
            "Iteration 13, Loss: 17.331019988822064\n",
            "Iteration 14, Loss: 16.306756707482677\n",
            "Iteration 15, Loss: 15.343027386070442\n",
            "Iteration 16, Loss: 14.43625446755368\n",
            "Iteration 17, Loss: 13.583071828521268\n",
            "Iteration 18, Loss: 12.780312283455652\n",
            "Iteration 19, Loss: 12.024995827503426\n",
            "Iteration 20, Loss: 11.314318574097976\n",
            "Iteration 21, Loss: 10.645642346368787\n",
            "Iteration 22, Loss: 10.016484883698395\n",
            "Iteration 23, Loss: 9.424510627071816\n",
            "Iteration 24, Loss: 8.867522049011871\n",
            "Iteration 25, Loss: 8.34345149591527\n",
            "Iteration 26, Loss: 7.850353512506679\n",
            "Iteration 27, Loss: 7.386397619917536\n",
            "Iteration 28, Loss: 6.949861520580408\n",
            "Iteration 29, Loss: 6.539124704714106\n",
            "Iteration 30, Loss: 6.152662434665503\n",
            "Iteration 31, Loss: 5.7890400847767705\n",
            "Iteration 32, Loss: 5.446907815766464\n",
            "Iteration 33, Loss: 5.124995563854671\n",
            "Iteration 34, Loss: 4.8221083260308575\n",
            "Iteration 35, Loss: 4.537121723962434\n",
            "Iteration 36, Loss: 4.268977830076255\n",
            "Iteration 37, Loss: 4.016681240318748\n",
            "Iteration 38, Loss: 3.7792953790159096\n",
            "Iteration 39, Loss: 3.5559390221160707\n",
            "Iteration 40, Loss: 3.345783025909011\n",
            "Iteration 41, Loss: 3.148047249077789\n",
            "Iteration 42, Loss: 2.9619976566572896\n",
            "Iteration 43, Loss: 2.786943595148845\n",
            "Iteration 44, Loss: 2.622235228675549\n",
            "Iteration 45, Loss: 2.4672611266608238\n",
            "Iteration 46, Loss: 2.321445994075166\n",
            "Iteration 47, Loss: 2.1842485358253256\n",
            "Iteration 48, Loss: 2.0551594473580463\n",
            "Iteration 49, Loss: 1.9336995240191863\n",
            "Iteration 50, Loss: 1.8194178821496518\n",
            "Iteration 51, Loss: 1.7118902853146067\n",
            "Iteration 52, Loss: 1.6107175694525138\n",
            "Iteration 53, Loss: 1.515524161097869\n",
            "Iteration 54, Loss: 1.4259566831769857\n",
            "Iteration 55, Loss: 1.3416826432012259\n",
            "Iteration 56, Loss: 1.2623891989880334\n",
            "Iteration 57, Loss: 1.18778199732784\n",
            "Iteration 58, Loss: 1.1175840812857634\n",
            "Iteration 59, Loss: 1.0515348620817766\n",
            "Iteration 60, Loss: 0.9893891517327431\n",
            "Iteration 61, Loss: 0.930916252865338\n",
            "Iteration 62, Loss: 0.8758991023209968\n",
            "Iteration 63, Loss: 0.8241334653738256\n",
            "Iteration 64, Loss: 0.7754271775702323\n",
            "Iteration 65, Loss: 0.7295994313758316\n",
            "Iteration 66, Loss: 0.6864801049815187\n",
            "Iteration 67, Loss: 0.6459091307771115\n",
            "Iteration 68, Loss: 0.6077359011481847\n",
            "Iteration 69, Loss: 0.571818709390327\n",
            "Iteration 70, Loss: 0.5380242236653578\n",
            "Iteration 71, Loss: 0.5062269920467349\n",
            "Iteration 72, Loss: 0.47630897681677353\n",
            "Iteration 73, Loss: 0.4481591162869011\n",
            "Iteration 74, Loss: 0.4216729125143454\n",
            "Iteration 75, Loss: 0.3967520433847474\n",
            "Iteration 76, Loss: 0.3733039976207088\n",
            "Iteration 77, Loss: 0.35124173136132436\n",
            "Iteration 78, Loss: 0.3304833450378702\n",
            "Iteration 79, Loss: 0.3109517793461322\n",
            "Iteration 80, Loss: 0.29257452918677535\n",
            "Iteration 81, Loss: 0.27528337451183676\n",
            "Iteration 82, Loss: 0.25901412707818716\n",
            "Iteration 83, Loss: 0.24370639216786655\n",
            "Iteration 84, Loss: 0.22930334439074554\n",
            "Iteration 85, Loss: 0.21575151673725296\n",
            "Iteration 86, Loss: 0.2030006020980815\n",
            "Iteration 87, Loss: 0.1910032665140846\n",
            "Iteration 88, Loss: 0.17971497346310225\n",
            "Iteration 89, Loss: 0.16909381853143338\n",
            "Iteration 90, Loss: 0.1591003738562249\n",
            "Iteration 91, Loss: 0.14969754176132236\n",
            "Iteration 92, Loss: 0.14085041704322837\n",
            "Iteration 93, Loss: 0.13252615739597357\n",
            "Iteration 94, Loss: 0.12469386149387143\n",
            "Iteration 95, Loss: 0.11732445427958357\n",
            "Iteration 96, Loss: 0.1103905790316602\n",
            "Iteration 97, Loss: 0.1038664958108892\n",
            "Iteration 98, Loss: 0.09772798590846558\n",
            "Iteration 99, Loss: 0.09195226194127534\n",
            "Iteration 100, Loss: 0.08651788326054576\n",
            "Iteration 101, Loss: 0.08140467635984766\n",
            "Iteration 102, Loss: 0.07659365998698062\n",
            "Iteration 103, Loss: 0.07206697468175022\n",
            "Iteration 104, Loss: 0.06780781647805834\n",
            "Iteration 105, Loss: 0.06380037452420508\n",
            "Iteration 106, Loss: 0.06002977238982451\n",
            "Iteration 107, Loss: 0.056482012841585764\n",
            "Iteration 108, Loss: 0.05314392588264784\n",
            "Iteration 109, Loss: 0.050003119862983315\n",
            "Iteration 110, Loss: 0.04704793547908108\n",
            "Iteration 111, Loss: 0.044267402492267266\n",
            "Iteration 112, Loss: 0.04165119900497404\n",
            "Iteration 113, Loss: 0.03918961314378035\n",
            "Iteration 114, Loss: 0.036873507006982977\n",
            "Iteration 115, Loss: 0.034694282742870286\n",
            "Iteration 116, Loss: 0.032643850632766785\n",
            "Iteration 117, Loss: 0.030714599060370322\n",
            "Iteration 118, Loss: 0.028899366255902493\n",
            "Iteration 119, Loss: 0.027191413710178584\n",
            "Iteration 120, Loss: 0.025584401159906914\n",
            "Iteration 121, Loss: 0.024072363051356495\n",
            "Iteration 122, Loss: 0.02264968639502138\n",
            "Iteration 123, Loss: 0.021311089929075627\n",
            "Iteration 124, Loss: 0.02005160451426725\n",
            "Iteration 125, Loss: 0.018866554687474075\n",
            "Iteration 126, Loss: 0.017751541305444478\n",
            "Iteration 127, Loss: 0.016702425214292563\n",
            "Iteration 128, Loss: 0.015715311884128023\n",
            "Iteration 129, Loss: 0.014786536951776086\n",
            "Iteration 130, Loss: 0.013912652617925996\n",
            "Iteration 131, Loss: 0.013090414848206543\n",
            "Iteration 132, Loss: 0.012316771330677616\n",
            "Iteration 133, Loss: 0.011588850145034585\n",
            "Iteration 134, Loss: 0.010903949101463065\n",
            "Iteration 135, Loss: 0.010259525709566468\n",
            "Iteration 136, Loss: 0.00965318774013127\n",
            "Iteration 137, Loss: 0.009082684344689475\n",
            "Iteration 138, Loss: 0.008545897699918217\n",
            "Iteration 139, Loss: 0.008040835145853157\n",
            "Iteration 140, Loss: 0.007565621788733219\n",
            "Iteration 141, Loss: 0.0071184935410191314\n",
            "Iteration 142, Loss: 0.0066977905727448606\n",
            "Iteration 143, Loss: 0.0063019511498957235\n",
            "Iteration 144, Loss: 0.0059295058369368625\n",
            "Iteration 145, Loss: 0.005579072041973911\n",
            "Iteration 146, Loss: 0.005249348884293189\n",
            "Iteration 147, Loss: 0.004939112365231465\n",
            "Iteration 148, Loss: 0.004647210824446307\n",
            "Iteration 149, Loss: 0.004372560664721486\n",
            "Iteration 150, Loss: 0.004114142329436494\n",
            "Iteration 151, Loss: 0.003870996517766834\n",
            "Iteration 152, Loss: 0.0036422206235667827\n",
            "Iteration 153, Loss: 0.003426965384714017\n",
            "Iteration 154, Loss: 0.0032244317304774505\n",
            "Iteration 155, Loss: 0.0030338678152062068\n",
            "Iteration 156, Loss: 0.0028545662273275238\n",
            "Iteration 157, Loss: 0.002685861363292443\n",
            "Iteration 158, Loss: 0.002527126956721865\n",
            "Iteration 159, Loss: 0.0023777737535795864\n",
            "Iteration 160, Loss: 0.00223724732474303\n",
            "Iteration 161, Loss: 0.0021050260078507234\n",
            "Iteration 162, Loss: 0.0019806189707867374\n",
            "Iteration 163, Loss: 0.0018635643896132343\n",
            "Iteration 164, Loss: 0.0017534277341871227\n",
            "Iteration 165, Loss: 0.001649800155096641\n",
            "Iteration 166, Loss: 0.0015522969659304577\n",
            "Iteration 167, Loss: 0.001460556215243966\n",
            "Iteration 168, Loss: 0.0013742373429230384\n",
            "Iteration 169, Loss: 0.0012930199159562866\n",
            "Iteration 170, Loss: 0.0012166024389232565\n",
            "Iteration 171, Loss: 0.0011447012347829176\n",
            "Iteration 172, Loss: 0.0010770493918072417\n",
            "Iteration 173, Loss: 0.0010133957727514104\n",
            "Iteration 174, Loss: 0.0009535040825818078\n",
            "Iteration 175, Loss: 0.0008971519913012032\n",
            "Iteration 176, Loss: 0.0008441303086153036\n",
            "Iteration 177, Loss: 0.0007942422073761319\n",
            "Iteration 178, Loss: 0.0007473024929201971\n",
            "Iteration 179, Loss: 0.0007031369155886336\n",
            "Iteration 180, Loss: 0.0006615815238773228\n",
            "Iteration 181, Loss: 0.0006224820558161947\n",
            "Iteration 182, Loss: 0.0005856933663174669\n",
            "Iteration 183, Loss: 0.0005510788883681015\n",
            "Iteration 184, Loss: 0.0005185101260655451\n",
            "Iteration 185, Loss: 0.00048786617761505856\n",
            "Iteration 186, Loss: 0.00045903328651801555\n",
            "Iteration 187, Loss: 0.0004319044192847635\n",
            "Iteration 188, Loss: 0.0004063788681050637\n",
            "Iteration 189, Loss: 0.0003823618770000461\n",
            "Iteration 190, Loss: 0.0003597642900693548\n",
            "Iteration 191, Loss: 0.0003385022205262612\n",
            "Iteration 192, Loss: 0.00031849673929316324\n",
            "Iteration 193, Loss: 0.0002996735820009465\n",
            "Iteration 194, Loss: 0.0002819628733046985\n",
            "Iteration 195, Loss: 0.0002652988674923804\n",
            "Iteration 196, Loss: 0.0002496197044235683\n",
            "Iteration 197, Loss: 0.00023486717989212869\n",
            "Iteration 198, Loss: 0.00022098652956051694\n",
            "Iteration 199, Loss: 0.0002079262256634926\n",
            "Iteration 200, Loss: 0.00019563778572677352\n",
            "Final weights:  [-3.3990955  -0.20180899  0.80271349]\n",
            "Final bias:  0.6009044964039992\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "weights = np.array([-3.0, -1.0, 2.0])\n",
        "bias = 1.0\n",
        "inputs = np.array([1.0, -2.0, 3.0])\n",
        "target_output = 0.0\n",
        "learning_rate = 0.001\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(x, 0)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return np.where(x > 0, 1, 0)\n",
        "\n",
        "for iteration in range(200):\n",
        "  #Forward pass\n",
        "  linear_output = np.dot(weights, inputs) + bias\n",
        "  output = relu(linear_output)\n",
        "  loss = (output - target_output) ** 2\n",
        "\n",
        "  #Backward pass\n",
        "  dloss_output = 2 * (output - target_output)\n",
        "  doutput_dlinear = relu_derivative(linear_output)\n",
        "  dlinear_dweights = inputs\n",
        "  dlinear_dbias = 1\n",
        "\n",
        "  dloss_dweights = dloss_output * doutput_dlinear * dlinear_dweights\n",
        "  dloss_dbias = dloss_output * doutput_dlinear * dlinear_dbias\n",
        "\n",
        "  #Update weights and biases\n",
        "  weights -= learning_rate * dloss_dweights\n",
        "  bias -= learning_rate * dloss_dbias\n",
        "\n",
        "  #Print loss for this iteration\n",
        "  print(f\"Iteration {iteration + 1}, Loss: {loss}\")\n",
        "\n",
        "print(\"Final weights: \",weights)\n",
        "print(\"Final bias: \",bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Back propagation in multiple neurons(layer)"
      ],
      "metadata": {
        "id": "ZsPKRR8NczAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Initial inputs\n",
        "inputs = np.array([1,2,3,4])\n",
        "\n",
        "# Initial weights and biases\n",
        "weights = np.array([\n",
        "    [0.1, 0.2, 0.3, 0.4],\n",
        "    [0.5, 0.6, 0.7, 0.8],\n",
        "    [0.9, 1.0, 1.1, 1.2]\n",
        "])\n",
        "\n",
        "biases = np.array([0.1, 0.2, 0.3])\n",
        "\n",
        "#learning rate\n",
        "learning_rate = 0.001\n",
        "\n",
        "#ReLU activation function and its derivative\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(x, 0)\n",
        "  # WRONG: np.max(x, 0) → takes max of entire array, returns ONE number\n",
        "  # Example:\n",
        "  # x = [2, -5, 4]\n",
        "  # np.max(x, 0) → max([2, -5, 4]) = 4 → max(4, 0) = 4  (single scalar)\n",
        "\n",
        "  # RIGHT: np.maximum(x, 0) → applies element-wise\n",
        "  # Example:\n",
        "  # x = [2, -5, 4]\n",
        "  # np.maximum(x, 0) → [2, 0, 4]  (correct ReLU)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return np.where(x>0, 1, 0)\n",
        "\n",
        "#Training loop\n",
        "for iteration in range(200):\n",
        "  #Forward pass\n",
        "  z = np.dot(weights, inputs) + biases\n",
        "  a = relu(z)\n",
        "  y = np.sum(a)\n",
        "\n",
        "  #Calculate loss\n",
        "  loss = y**2\n",
        "\n",
        "  #Backward pass\n",
        "  #Gradient of loss with respect to output y\n",
        "  dL_dy = 2*y\n",
        "\n",
        "  #Gradient of y with respect to a\n",
        "  dy_da = np.ones_like(a)\n",
        "\n",
        "  #Gradient of a with respect to z(ReLU derivative)\n",
        "  da_dz = relu_derivative(z)\n",
        "\n",
        "  #Gradient of z with respect to weights and biases\n",
        "  dL_dz = dL_dy * dy_da * da_dz            # <-- Added missing step\n",
        "  dL_dW = np.outer(dL_dz, inputs)\n",
        "  dL_dB = dL_dz\n",
        "\n",
        "  # Update weights and biases\n",
        "  weights -= learning_rate * dL_dW\n",
        "  biases -= learning_rate * dL_dB\n",
        "\n",
        "  # Print the loss every 20 iterations\n",
        "  if iteration % 20 == 0:\n",
        "    print(f\"Iteration {iteration}, Loss: {loss}\")\n",
        "\n",
        "# Final weights and biases\n",
        "print(\"Final weights:\\n\", weights)\n",
        "print(\"Final biases:\\n\", biases)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBw-CEifcylw",
        "outputId": "17c6f1f4-d425-4227-a296-56cf1bcdf668"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 466.56000000000006\n",
            "Iteration 20, Loss: 5.32959636083938\n",
            "Iteration 40, Loss: 0.41191523404899866\n",
            "Iteration 60, Loss: 0.031836212079467595\n",
            "Iteration 80, Loss: 0.002460565465389601\n",
            "Iteration 100, Loss: 0.000190172825660145\n",
            "Iteration 120, Loss: 1.4698126966451542e-05\n",
            "Iteration 140, Loss: 1.1359926717815175e-06\n",
            "Iteration 160, Loss: 8.779889800154524e-08\n",
            "Iteration 180, Loss: 6.7858241357822796e-09\n",
            "Final weights:\n",
            " [[-0.00698895 -0.01397789 -0.02096684 -0.02795579]\n",
            " [ 0.25975286  0.11950572 -0.02074143 -0.16098857]\n",
            " [ 0.53548461  0.27096922  0.00645383 -0.25806156]]\n",
            "Final biases:\n",
            " [-0.00698895 -0.04024714 -0.06451539]\n"
          ]
        }
      ]
    }
  ]
}